Minimal HTTP inference server in OpenAI API[^1].

> _When you don't want to install countless frameworks, generators, etc. When all you need is small Docker file and single main for http server._

> [!WARNING]  
> Under development. Limited OpenAPI API compatibility.

- 100 lines of code
- CUDA
- Pytorch
- HuggingFace models (e.g. Llama 3.2 11B Vision)
- OpenTelemetry

[^1]: https://github.com/openai/openai-openapi
